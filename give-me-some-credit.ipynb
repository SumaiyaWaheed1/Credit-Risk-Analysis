{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3268,"sourceType":"datasetVersion","datasetId":1889}],"dockerImageVersionId":648,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"../input\"))\n#Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:04:14.233879Z","iopub.execute_input":"2025-04-20T12:04:14.234165Z","iopub.status.idle":"2025-04-20T12:04:14.243599Z","shell.execute_reply.started":"2025-04-20T12:04:14.234106Z","shell.execute_reply":"2025-04-20T12:04:14.242731Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#test_data = pd.read_csv(\"../input/cs-test.csv\")\n#train_data = pd.read_csv(\"../input/cs-training.csv\")\ntest_data = pd.read_csv(\"../input/cs-test.csv\")\ntrain_data = pd.read_csv(\"../input/cs-training.csv\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:04:32.085320Z","iopub.execute_input":"2025-04-20T12:04:32.085597Z","iopub.status.idle":"2025-04-20T12:04:32.317843Z","shell.execute_reply.started":"2025-04-20T12:04:32.085542Z","shell.execute_reply":"2025-04-20T12:04:32.317211Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"test_data.sample(10)\n","metadata":{"_uuid":"d11d6e3913f5c2fddf4d57576766620d0309cd16","_cell_guid":"e890da31-fb32-4f2c-b4f6-f697e78f2115","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:04:38.829126Z","iopub.execute_input":"2025-04-20T12:04:38.829377Z","iopub.status.idle":"2025-04-20T12:04:38.858282Z","shell.execute_reply.started":"2025-04-20T12:04:38.829342Z","shell.execute_reply":"2025-04-20T12:04:38.857642Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       Unnamed: 0  SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  \\\n20965       20966               NaN                              0.021788   \n9162         9163               NaN                              0.067046   \n55594       55595               NaN                              0.058907   \n62780       62781               NaN                              0.510580   \n63743       63744               NaN                              0.893858   \n98692       98693               NaN                              0.039673   \n39572       39573               NaN                              0.001425   \n58329       58330               NaN                              0.348217   \n82553       82554               NaN                              0.043242   \n77963       77964               NaN                              0.060626   \n\n       age  NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n20965   80                                     0   715.000000            NaN   \n9162    36                                     0  3175.000000            NaN   \n55594   65                                     0     0.140400         7150.0   \n62780   46                                     0     0.304205        11866.0   \n63743   44                                     2     0.962679         3000.0   \n98692   50                                     0     0.293658         7300.0   \n39572   68                                     0  1619.000000            NaN   \n58329   36                                     0     0.221395         4701.0   \n82553   62                                     0     0.269097         5157.0   \n77963   56                                     0     0.000484         4128.0   \n\n       NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n20965                               15                        0   \n9162                                 6                        0   \n55594                                8                        0   \n62780                               16                        0   \n63743                                8                        0   \n98692                               10                        0   \n39572                                7                        0   \n58329                                6                        0   \n82553                               10                        0   \n77963                                2                        0   \n\n       NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n20965                             1                                     0   \n9162                              2                                     0   \n55594                             2                                     0   \n62780                             1                                     0   \n63743                             1                                     0   \n98692                             1                                     0   \n39572                             2                                     0   \n58329                             0                                     0   \n82553                             1                                     0   \n77963                             0                                     0   \n\n       NumberOfDependents  \n20965                 0.0  \n9162                  NaN  \n55594                 0.0  \n62780                 0.0  \n63743                 2.0  \n98692                 3.0  \n39572                 0.0  \n58329                 0.0  \n82553                 1.0  \n77963                 0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>SeriousDlqin2yrs</th>\n      <th>RevolvingUtilizationOfUnsecuredLines</th>\n      <th>age</th>\n      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n      <th>DebtRatio</th>\n      <th>MonthlyIncome</th>\n      <th>NumberOfOpenCreditLinesAndLoans</th>\n      <th>NumberOfTimes90DaysLate</th>\n      <th>NumberRealEstateLoansOrLines</th>\n      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n      <th>NumberOfDependents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20965</th>\n      <td>20966</td>\n      <td>NaN</td>\n      <td>0.021788</td>\n      <td>80</td>\n      <td>0</td>\n      <td>715.000000</td>\n      <td>NaN</td>\n      <td>15</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9162</th>\n      <td>9163</td>\n      <td>NaN</td>\n      <td>0.067046</td>\n      <td>36</td>\n      <td>0</td>\n      <td>3175.000000</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>55594</th>\n      <td>55595</td>\n      <td>NaN</td>\n      <td>0.058907</td>\n      <td>65</td>\n      <td>0</td>\n      <td>0.140400</td>\n      <td>7150.0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>62780</th>\n      <td>62781</td>\n      <td>NaN</td>\n      <td>0.510580</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0.304205</td>\n      <td>11866.0</td>\n      <td>16</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>63743</th>\n      <td>63744</td>\n      <td>NaN</td>\n      <td>0.893858</td>\n      <td>44</td>\n      <td>2</td>\n      <td>0.962679</td>\n      <td>3000.0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>98692</th>\n      <td>98693</td>\n      <td>NaN</td>\n      <td>0.039673</td>\n      <td>50</td>\n      <td>0</td>\n      <td>0.293658</td>\n      <td>7300.0</td>\n      <td>10</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>39572</th>\n      <td>39573</td>\n      <td>NaN</td>\n      <td>0.001425</td>\n      <td>68</td>\n      <td>0</td>\n      <td>1619.000000</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>58329</th>\n      <td>58330</td>\n      <td>NaN</td>\n      <td>0.348217</td>\n      <td>36</td>\n      <td>0</td>\n      <td>0.221395</td>\n      <td>4701.0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>82553</th>\n      <td>82554</td>\n      <td>NaN</td>\n      <td>0.043242</td>\n      <td>62</td>\n      <td>0</td>\n      <td>0.269097</td>\n      <td>5157.0</td>\n      <td>10</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>77963</th>\n      <td>77964</td>\n      <td>NaN</td>\n      <td>0.060626</td>\n      <td>56</td>\n      <td>0</td>\n      <td>0.000484</td>\n      <td>4128.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"#Let's take a look at the data\ntrain_data.sample(10)","metadata":{"_uuid":"79fb3ce439caf55073bc469b928915795059dd9d","collapsed":true,"_cell_guid":"88d02f43-9c9e-47fb-8078-c397d2ce2501","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.info()","metadata":{"_uuid":"5ce18f6f0a7ae36b4db1318566cff633e670f2d0","collapsed":true,"_cell_guid":"90ab7287-5ef1-47ce-a19d-723c28ea71a2","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have large null values for MonthlyIncome and NumberOfDependents, we will handle those values in a bit. Also, these features has inconsistent data types, we will change them to int64. Let's look at the summary statistics of the features.","metadata":{"_uuid":"cf1634f814dd1b1e6ba2d28fab1cb8a54c0f4ee9","collapsed":true,"_cell_guid":"347c33a8-ac63-4e61-8765-e4f1c4e2a27c","jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"train_data.describe()","metadata":{"_uuid":"d926de362e6e21630e05d2c40c44b45b37f0405e","collapsed":true,"_cell_guid":"f0bb21fe-956a-47e5-8ac9-69563f3d51df","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Age feature seems to have an outlier value 0. I assume that it is not recorded and we will impute it also with the age's median.  Meanwhile, The features NumberOfTimes90DaysLate, NumberOfTime60-89DaysPastDueNotWorse\tand NumberOfTime30-59DaysPastDueNotWorse looks like giving the same information. Also, NumberOfOpenCreditLinesAndLoans and NumberRealEstateLoansOrLines. We will check their correlations to each other with the correlation matrix and do something to make use of these features. Let's go now and have a sneak peek on our test data.","metadata":{"_uuid":"95d97e37b55bde22b63f39cb440996f7de69a9b3","_cell_guid":"7232aeab-046f-4f6d-b784-f8e52dd9b5b4"}},{"cell_type":"code","source":"test_data.sample(10)","metadata":{"_uuid":"5548b6e04bc3d522577b85d5297efc0b8b0ea3cc","collapsed":true,"_cell_guid":"affd4e0c-c8b7-4e30-b92b-0f6797081562","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.info()","metadata":{"_uuid":"e1260edccd9224bba6943e03333498bac5462c1e","collapsed":true,"_cell_guid":"ec1058e2-b072-418d-af2e-59f9cd582843","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.describe()","metadata":{"_uuid":"339aa189ebfca3064513797af23f41105afe0030","collapsed":true,"_cell_guid":"77b5d0d6-367e-4fb8-a7c1-e6a842a0c729","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Unlike our training dataset, our test set the min value of the age feature is 21. The MonthlyIncome and NumberOfDependents feature also has null values and we will handle those accordingly. Ofcourse SeriousDlqin2yrs has 0 values since it is our target class.","metadata":{"_uuid":"7a2e49fbfd33635b63ff29ed1ab1a8e1287804c8","_cell_guid":"8d293e33-179f-4357-b1f9-4eaa7244572f"}},{"cell_type":"markdown","source":"**Let's get our hands dirty!**","metadata":{"_uuid":"6079dff36244c0f979c32bc0e1fd9de4500d3e4f","_cell_guid":"78984544-3585-4510-9152-fde10c9f75d8"}},{"cell_type":"markdown","source":"First, we will look at the distribution of our target class. SeriousDlqin2yrs to have some perspective about the problem.","metadata":{"_uuid":"ea1d4dc1c6d0bc351682ef15ff69952f40ca38db","_cell_guid":"7f247009-cdf9-4ce6-b649-aeb3725acde4"}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.countplot(\"SeriousDlqin2yrs\", data=train_data)","metadata":{"_uuid":"2c212730f97866dfc2ebd92ad73f1c223bf3c837","collapsed":true,"_cell_guid":"8e02f23b-df15-4838-8990-0e012038d3f7","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is clear problem here, we have an **unbalanced target class!!** we will check the event rate of financial distress (SeriousDlqin2yrs) in our dataset.","metadata":{"_uuid":"1fd34e34a8d36004d337fc09c2c9b6b029a0a348","_cell_guid":"8439b7b7-2e26-44f0-982e-5aafdb8680d3"}},{"cell_type":"code","source":"class_0 = train_data.SeriousDlqin2yrs.value_counts()[0]\nclass_1 = train_data.SeriousDlqin2yrs.value_counts()[1]\nprint(\"Total number of class_0: {}\".format(class_0))\nprint(\"Total number of class_1: {}\".format(class_1))\nprint(\"Event rate: {} %\".format(class_1/(class_0+class_1) *100))","metadata":{"_uuid":"fa9e411b45b9e777a0f4300b0f221ca505d5f174","collapsed":true,"_cell_guid":"7d3fd16f-efa7-446e-b04a-085673b97955","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have an event rate of **6.68%**, consequences of having this kind of target class is most likely that the minority class is being ignored by the algorithm and will predict the new instances to class_0 as it was the safest way to have a great accuracy.\nThere are guides on how to handle this problem and what I found most useful was the article of Jason Brownlee [here.](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/) This competition uses an evaluation metric AUC so we will work inline with this evaluation metric (i.e using ROC Curve to compare models).\nAfter reading the article I have concluded ways on how to tackle the problem.\n1.  Since we have a lot of data, over 100,000 training data set, we will consider using Resampling (Under-sampling to be exact) this strategy will randomly delete some of the instances of the majority class (class_0) to make it balanced. using the [imbalanced-learn module](https://github.com/scikit-learn-contrib/imbalanced-learn)\n2. Using penalized models (penalized RF, Logit)\n3. Considering ensemble models.\n","metadata":{"_uuid":"327f65b2394a319271ea8350cfa5cf687e555653","_cell_guid":"c322686f-40d8-4c67-933f-2b79149b5e4c"}},{"cell_type":"markdown","source":"Back to the training set.\nage feature has a 0 value in it, so we will locate the entry and impute it with the age median.","metadata":{"_uuid":"bbd98c9bf8d8dae421dd1083d2f3532210e533bb","_cell_guid":"deaabd15-8c0a-43a7-ac8c-80449b99eeba"}},{"cell_type":"code","source":"train_data.loc[train_data[\"age\"] < 18] #less than legal age","metadata":{"_uuid":"6f80d6427d36c616ec30e3c2cb59f22e43146b74","collapsed":true,"_cell_guid":"93946aaf-af1e-44d6-a3f1-464724f1d442","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"only one instance, let's impute it right away.","metadata":{"_uuid":"41fc026d524e70db06d51471922a9fdc5f5fca81","_cell_guid":"32a990d0-1365-4f5a-867f-1b81c8705e17"}},{"cell_type":"code","source":"train_data.loc[train_data[\"age\"] == 0, \"age\"] = train_data.age.median()","metadata":{"_uuid":"2d970f9c81ccc3878692e44d30cbdd07f075bf09","collapsed":true,"_cell_guid":"8ae449d2-8382-4321-be17-d3c79904887e","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We're done with the age feature, now we will go and impute missing values for the MonthlyIncome feature. We will tackle this differently by bracketing ages with the working (18 to 60) and senior (60 and above). First, let's create a temporary dataframes for them and compute for each's median then impute the values accordingly.","metadata":{"_uuid":"7fc675ee918f3889f7e8cb835f9f34dabc7f445f","_cell_guid":"b2cd5e9b-5d3b-4e07-8c46-33f1ef5f1243"}},{"cell_type":"code","source":"age_working = train_data.loc[(train_data[\"age\"] >= 18) & (train_data[\"age\"] < 60)]\nage_senior = train_data.loc[(train_data[\"age\"] >= 60)]\n\nage_working_impute = age_working.MonthlyIncome.mean()\nage_senior_impute = age_senior.MonthlyIncome.mean()","metadata":{"_uuid":"da79376846320dcfcaef672c409115d9e7495b05","collapsed":true,"_cell_guid":"6c78330e-7e4d-4568-9211-cbe2efcce473","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will change the monthlyincome data type to int64 then fill those null values with 99999 and impute with the corresponding age's monthlyincome mean.","metadata":{"_uuid":"8624282d19ec7144cf00ac4d7f252ffe870c63a3","_cell_guid":"46a06a33-06e2-41f8-94f1-238968516dc0"}},{"cell_type":"code","source":"train_data[\"MonthlyIncome\"] = np.absolute(train_data[\"MonthlyIncome\"])","metadata":{"_uuid":"a46773baaee2f0d4b558f64bbcef8b2892046f2e","collapsed":true,"_cell_guid":"ff5bbd24-e19f-4da0-a117-a1b5f07307b0","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"MonthlyIncome\"] = train_data[\"MonthlyIncome\"].fillna(99999)","metadata":{"_uuid":"6165a3d42e81e334b8e31ffeed2d48304c5e9c45","collapsed":true,"_cell_guid":"53aa5799-62ad-4ac9-bbf5-cd65f4974677","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"MonthlyIncome\"] = train_data[\"MonthlyIncome\"].astype('int64')","metadata":{"_uuid":"065b69b88213c4bc52b0bdff9a7fa814ff10467d","collapsed":true,"_cell_guid":"3fbe01bf-1ef5-4172-af65-7953f94388d0","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.loc[((train_data[\"age\"] >= 18) & (train_data[\"age\"] < 60)) & (train_data[\"MonthlyIncome\"] == 99999),\\\n               \"MonthlyIncome\"] = age_working_impute\ntrain_data.loc[(train_data[\"age\"] >= 60) & (train_data[\"MonthlyIncome\"] == 99999), \"MonthlyIncome\"] = age_senior_impute","metadata":{"_uuid":"f3e90038b006d37d563a860d1ab4537244e0a513","collapsed":true,"_cell_guid":"d2df094b-084c-44ae-afb0-cf11b4fd434e","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check\ntrain_data.info()","metadata":{"_uuid":"d7dd140e7cb1eabbfa21beb8384363ea8cbca8b7","collapsed":true,"_cell_guid":"c9f28478-5d77-4030-a743-4dbb38ccef41","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.loc[train_data[\"MonthlyIncome\"] == 99999]","metadata":{"_uuid":"ee79cd3224a727469f8ba0d7c95569a597f1de6d","collapsed":true,"_cell_guid":"8c7e8129-d40e-43d1-9ae7-e236eef9b951","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We're done with the Monthly Income, now we will move to the NumberOfDependents feature.","metadata":{"_uuid":"571550a711c1caca3f6ac6a1186200ffac277fdc","collapsed":true,"_cell_guid":"68212a58-7b40-49de-a33a-26f86e2c4526","jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"train_data[\"NumberOfDependents\"] = np.absolute(train_data[\"NumberOfDependents\"])\ntrain_data[\"NumberOfDependents\"] = train_data[\"NumberOfDependents\"].fillna(0)\ntrain_data[\"NumberOfDependents\"] = train_data[\"NumberOfDependents\"].astype('int64')","metadata":{"_uuid":"dc7dab13f7d669ce92ddcf1b6541a180fef53fcb","collapsed":true,"_cell_guid":"50789103-19a6-41b8-917b-d4601459ad70","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.NumberOfDependents.value_counts()","metadata":{"_uuid":"51d07f7ce77c0eca76bdf14c6ec0fa9c57ca869d","collapsed":true,"_cell_guid":"618afda0-7c5c-49a6-9cad-64afebb20acd","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I decided not to go through each of the numberofdependents feature and impute it by the mode. We will now take a look at the correlation of the features to the target variable.","metadata":{"_uuid":"221d5fec8b74f0336ddb11f64dac7f5831ad7bbe","collapsed":true,"_cell_guid":"fbdface7-8f79-43aa-b05e-a3d3155d05e2","jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"corr = train_data.corr()\nplt.figure(figsize=(14,12))\nsns.heatmap(corr, annot=True, fmt=\".2g\")","metadata":{"_uuid":"17ca5cf5e1a37aac1b5a076cdb7c093bb5e119e8","collapsed":true,"_cell_guid":"3da1fe1d-6eef-417c-a155-f3058d020f3c","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Findings**: As expected, the NumberOfTimes90DaysLate, NumberOfTime60-89DaysPastDueNotWorse and NumberOfTime30-59DaysPastDueNotWorse are highly correlated to each other and keeping all those features won't help the prediction power of algorithms(avoiding multicollinearity). I came up with 2 ways to handle this, drop the other 2 features and keep 1 or combine the three features and make a binary feature that classify if a borrower defaulted any loan/credit payment. Also, the NumberOfOpenCreditLinesAndLoans and NumberRealEstateLoansOrLines features are somehow correlated to each other but has different degree of correlation from our target class we can also handle this features the same way as we will handle the pastdue/late features.","metadata":{"_uuid":"722e9cf769357af6303ea009a96d1538fc8705a3","_cell_guid":"4d601fe4-7fd0-496d-a889-2d1398a712e2"}},{"cell_type":"markdown","source":"We will go with feature engineering the pastdue/late features (because a default is a default!) and credit/loans features but providing a buffer since debts are everywhere!","metadata":{"_uuid":"518583d1b9c7e22f5846c88eacd566889ff044b2","_cell_guid":"23210c05-9cc5-48b0-b6e6-300f1c9322dc"}},{"cell_type":"code","source":"train_data[\"CombinedDefaulted\"] = (train_data[\"NumberOfTimes90DaysLate\"] + train_data[\"NumberOfTime60-89DaysPastDueNotWorse\"])\\\n                                        + train_data[\"NumberOfTime30-59DaysPastDueNotWorse\"]","metadata":{"_uuid":"79a469d9df69d10885aac1c5653c69e677e8a147","collapsed":true,"_cell_guid":"7b5656d8-d4c1-42bf-8626-9e6326f7b038","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.loc[(train_data[\"CombinedDefaulted\"] >= 1), \"CombinedDefaulted\"] = 1","metadata":{"_uuid":"e7456be94d16a6286120144a3c2ada6813720f5a","collapsed":true,"_cell_guid":"fbb77d78-3b18-42d1-b335-3f6b265ec63b","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"CombinedCreditLoans\"] = train_data[\"NumberOfOpenCreditLinesAndLoans\"] + \\\n                                        train_data[\"NumberRealEstateLoansOrLines\"]","metadata":{"_uuid":"7075fb167d1751bc26fd1030a670f33a6a5eeeee","collapsed":true,"_cell_guid":"39ab2d3d-77c4-40b5-ba67-75f702843886","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.loc[(train_data[\"CombinedCreditLoans\"] <= 5), \"CombinedCreditLoans\"] = 0\ntrain_data.loc[(train_data[\"CombinedCreditLoans\"] > 5), \"CombinedCreditLoans\"] = 1","metadata":{"_uuid":"f7044076a1ab4fab4a2b83daf0a9a500d7b4312f","collapsed":true,"_cell_guid":"1f49bd19-3638-4b70-8d0d-89cc12c58890","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.CombinedCreditLoans.value_counts()","metadata":{"_uuid":"326cc4db54aeb72a75bc4555d7ad29f7c12667af","collapsed":true,"_cell_guid":"e699d7f3-beac-41f8-93c3-a28524831db6","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we will create a binary feature WithDependents which is derived from the NumberOfDependents feature. Also, from the description of the data DebtRatio = Monthly debt payments / monthly gross income. we will extract MonthlyDebtPayments from this formula to get a new feature.","metadata":{"_uuid":"62b3ab2b4923efe07fe5d86e4a0472bbf7d6657c","_cell_guid":"2519acc5-a6d5-4036-89c9-ac69300469d1"}},{"cell_type":"code","source":"train_data[\"WithDependents\"] = train_data[\"NumberOfDependents\"]\ntrain_data.loc[(train_data[\"WithDependents\"] >= 1), \"WithDependents\"] = 1","metadata":{"_uuid":"7677653d891abd7822e47061a01cb0178c42ec56","collapsed":true,"_cell_guid":"6fa8f1bc-149d-450f-9cae-da4b218305c8","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.WithDependents.value_counts()","metadata":{"_uuid":"9ee8e5fce3fe5ac8e3d935bf77cb6c2b53edc67f","collapsed":true,"_cell_guid":"aca0e6bd-ef54-471f-b68c-86c64689ee42","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"MonthlyDebtPayments\"] = train_data[\"DebtRatio\"] * train_data[\"MonthlyIncome\"]\ntrain_data[\"MonthlyDebtPayments\"] = np.absolute(train_data[\"MonthlyDebtPayments\"])\ntrain_data[\"MonthlyDebtPayments\"] = train_data[\"MonthlyDebtPayments\"].astype('int64')","metadata":{"_uuid":"12c26116936dfb4fe953476ab4a3c2814b56d087","collapsed":true,"_cell_guid":"f4ca1fdb-9af3-4737-8116-0a82aba64e06","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"age\"] = train_data[\"age\"].astype('int64')\ntrain_data[\"MonthlyIncome\"] = train_data[\"MonthlyIncome\"].astype('int64')","metadata":{"_uuid":"58b301526b10501ef6a75ddc2e9e3e5b8cb8c7fc","collapsed":true,"_cell_guid":"b0dbf526-0a8f-4ac9-b9d2-a8816fc68bcd","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Also, let's see if we can get a good predictor out of age feature. using senior and working temporary dataframes earlier.","metadata":{"_uuid":"7fcb8fa95fd4aa70a531a90a095e39cb0affc9e1","_cell_guid":"5a875b3d-0a8e-4007-bb4a-7debd77e6fb6"}},{"cell_type":"code","source":"train_data[\"age_map\"] = train_data[\"age\"]\ntrain_data.loc[(train_data[\"age\"] >= 18) & (train_data[\"age\"] < 60), \"age_map\"] = 1\ntrain_data.loc[(train_data[\"age\"] >= 60), \"age_map\"] = 0 ","metadata":{"_uuid":"ff7ae9d6acbf409818c5741b6c400a214fd75777","collapsed":true,"_cell_guid":"3d7d2271-9706-4736-b02e-c77fac2c239d","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#replacing those numbers to categorical features then get the dummy variables\ntrain_data[\"age_map\"] = train_data[\"age_map\"].replace(0, \"working\")\ntrain_data[\"age_map\"] = train_data[\"age_map\"].replace(1, \"senior\")","metadata":{"_uuid":"349d44ea180bf9a3513a9525944fa078649454fb","collapsed":true,"_cell_guid":"fe0153b0-2afd-4e6c-a2db-5e050bca25d8","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.concat([train_data, pd.get_dummies(train_data.age_map,prefix='is')], axis=1)","metadata":{"_uuid":"ce68618409bd1b90af5efe174ddadc689d93b6a3","collapsed":true,"_cell_guid":"490d4747-a4d5-466e-ab18-6b94724e4c51","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's look at the correlation matrix to decide to retain or drop the engineered features (avoiding multicollinearity).","metadata":{"_uuid":"5dc143b5ca389e35e4691151cd2e5d5e61680729","_cell_guid":"5290867c-b85f-4f71-8f1b-16ee3676ae93"}},{"cell_type":"code","source":"corr = train_data.corr()\nplt.figure(figsize=(14,12))\nsns.heatmap(corr, annot=True, fmt=\".2g\")","metadata":{"_uuid":"c656c97eef2e26ebb1f63dfd10577dba7bb5092f","collapsed":true,"_cell_guid":"bec1d67e-855d-4138-9089-ebf848622bcf","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Findings: \n* we will retain CombinedDefaulted feature as it clearly a good predictor of our target class than the three features it was derived from.\n* we will retain NumberOfTime30-59DaysPastDueNotWorse and drop the other two features derived from CombinedDefaulted as it gives a more meaningful information on our target variable (also, it looks like this is the medium range of time a borrower defaulted a payment)\n* we will drop the engineered is_working and is_senior feature since age feature outperforms them.\n* we will drop also the WithDependents\n* we will retain CombinedCreditLoans also since it outperforms the two features it came from.\n* we will drop MonthlyDebtPayments\n","metadata":{"_uuid":"bb957925aacc25076d3873707d7de5224364db50","collapsed":true,"_cell_guid":"aafaddc8-a911-4c60-b740-30243e9a8178","jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"train_data.columns","metadata":{"_uuid":"6a7e67d288825edfed227d59512c9c7e260d39be","collapsed":true,"_cell_guid":"47ae5d2c-1202-4570-8c80-dc7347d1e9ce","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.drop([\"Unnamed: 0\",\"NumberOfOpenCreditLinesAndLoans\",\\\n                 \"NumberOfTimes90DaysLate\",\"NumberRealEstateLoansOrLines\",\"NumberOfTime60-89DaysPastDueNotWorse\",\\\n                 \"WithDependents\",\"age_map\",\"is_senior\",\"is_working\", \"MonthlyDebtPayments\"], axis=1, inplace=True)","metadata":{"_uuid":"cedbc232746852c56c28199478679a5701ef0231","collapsed":true,"_cell_guid":"9521fff6-295c-4555-ad79-b88b9dd02ed4","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.columns","metadata":{"_uuid":"46c4128a93a6af5063465b67ace4a5e1c9757c6e","collapsed":true,"_cell_guid":"a0f21ade-1dd7-4b97-ac16-663137375bf7","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#now let's take a look at the filtered final features to be used in predicting the financial distress for the next two years\ncorr = train_data.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, annot=True, fmt=\".2g\")","metadata":{"_uuid":"69fd291ca6f3401650925844dce70f15f0b5df16","collapsed":true,"_cell_guid":"ecc00867-54af-49d2-929d-4a7429c72b18","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ta-da! we now have clean training dataset. now it's ready to apply algorithms to it but before that, since we have unbalanced dataset we know that this will not generalized well in the test set. So, we need to perform the undersampling or penalized kernels as we shortlisted those strategies earlier.","metadata":{"_uuid":"3fb93ea43a6aaed39e8cb19c015b215262c0cba0","collapsed":true,"_cell_guid":"4205ed22-7f5b-49b7-a427-d67ff2087e03","jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"Now let's also clean the test set! Since we have concluded what features to retain and drop. we will skip some of the process.","metadata":{"_uuid":"68eb351fd0b32cbdce4791b3fd39836c2209d87d","_cell_guid":"97d8b9b6-b849-45ca-ac1d-b5936d01f43c"}},{"cell_type":"code","source":"def cleaned_dataset(dataset):\n    dataset.loc[dataset[\"age\"] <= 18, \"age\"] = dataset.age.median()\n    \n    age_working = dataset.loc[(dataset[\"age\"] >= 18) & (dataset[\"age\"] < 60)]\n    age_senior = dataset.loc[(dataset[\"age\"] >= 60)]\n\n    age_working_impute = age_working.MonthlyIncome.mean()\n    age_senior_impute = age_senior.MonthlyIncome.mean()\n\n    dataset[\"MonthlyIncome\"] = np.absolute(dataset[\"MonthlyIncome\"])\n    dataset[\"MonthlyIncome\"] = dataset[\"MonthlyIncome\"].fillna(99999)\n    dataset[\"MonthlyIncome\"] = dataset[\"MonthlyIncome\"].astype('int64')\n\n    dataset.loc[((dataset[\"age\"] >= 18) & (dataset[\"age\"] < 60)) & (dataset[\"MonthlyIncome\"] == 99999),\\\n                   \"MonthlyIncome\"] = age_working_impute\n    dataset.loc[(train_data[\"age\"] >= 60) & (dataset[\"MonthlyIncome\"] == 99999), \"MonthlyIncome\"] = age_senior_impute\n    dataset[\"NumberOfDependents\"] = np.absolute(dataset[\"NumberOfDependents\"])\n    dataset[\"NumberOfDependents\"] = dataset[\"NumberOfDependents\"].fillna(0)\n    dataset[\"NumberOfDependents\"] = dataset[\"NumberOfDependents\"].astype('int64')\n\n    dataset[\"CombinedDefaulted\"] = (dataset[\"NumberOfTimes90DaysLate\"] + dataset[\"NumberOfTime60-89DaysPastDueNotWorse\"])\\\n                                            + dataset[\"NumberOfTime30-59DaysPastDueNotWorse\"]\n\n    dataset.loc[(dataset[\"CombinedDefaulted\"] >= 1), \"CombinedDefaulted\"] = 1\n\n    dataset[\"CombinedCreditLoans\"] = dataset[\"NumberOfOpenCreditLinesAndLoans\"] + \\\n                                            dataset[\"NumberRealEstateLoansOrLines\"]\n    dataset.loc[(dataset[\"CombinedCreditLoans\"] <= 5), \"CombinedCreditLoans\"] = 0\n    dataset.loc[(dataset[\"CombinedCreditLoans\"] > 5), \"CombinedCreditLoans\"] = 1\n\n    dataset.drop([\"Unnamed: 0\",\"NumberOfOpenCreditLinesAndLoans\",\\\n                 \"NumberOfTimes90DaysLate\",\"NumberRealEstateLoansOrLines\",\"NumberOfTime60-89DaysPastDueNotWorse\"], axis=1, inplace=True)\n\ncleaned_dataset(test_data)","metadata":{"_uuid":"5991a9c53d0a51f93eb79f91cea239cf4e67706a","collapsed":true,"_cell_guid":"2cdf2ad9-2d49-4570-acbe-ba81c665ae54","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.columns","metadata":{"_uuid":"ddcc84105fa9efd99f5aad47b23b75509bc91d3d","collapsed":true,"_cell_guid":"0434f461-66dc-4b7c-94cc-4eb605f8d3b9","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.columns","metadata":{"_uuid":"446f53d1032ea0d0cd577388f326089c454cfe05","collapsed":true,"_cell_guid":"d6b80c60-40b3-4c2a-b482-048099ce46c8","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.shape, test_data.shape","metadata":{"_uuid":"4199a8213e253d5f1dbfc3dd16001f1dfd5b2636","collapsed":true,"_cell_guid":"deb9c40c-34b4-4c05-b0ff-1355a914ce32","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.info()","metadata":{"_uuid":"83f893d42ea8e71f81f6e2cc21998b6b6e342947","collapsed":true,"_cell_guid":"aeb393ad-fbc5-49fb-a132-37e8552f77e5","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Let's split our predictors and the target variable in our datasets\nX = train_data.drop(\"SeriousDlqin2yrs\", axis=1).copy()\ny = train_data.SeriousDlqin2yrs\nX.shape, y.shape","metadata":{"_uuid":"6995a0d835dfc7518767a9ed213a6ff5f73ece41","collapsed":true,"_cell_guid":"ff80d522-4430-4473-a6a2-da8e6238b662","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = test_data.drop(\"SeriousDlqin2yrs\", axis=1).copy()\ny_test = test_data.SeriousDlqin2yrs\nX_test.shape, y_test.shape","metadata":{"_uuid":"342fdff2d79c14214371d92725a916d983d048f5","collapsed":true,"_cell_guid":"c11d38bf-1d90-40b1-b59d-817af263d614","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#let's first try the penalized model Logit by providing the class_weight=\"balanced\" parameter\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_predict #to perform stratified sampling using cv param\nfrom sklearn.metrics import roc_curve, roc_auc_score #AUC score\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_val, y_train, y_val = train_test_split(X,y,random_state=42)\nlogit = LogisticRegression(random_state=42, solver=\"saga\", penalty=\"l1\", class_weight=\"balanced\", C=1.0, max_iter=500)\nscaler = StandardScaler().fit(X_train)","metadata":{"_uuid":"c4a385ac8ee2d0e0a6809caa323f6cd6de5db41c","collapsed":true,"_cell_guid":"d24af103-925b-4329-bbc7-3ce80adef140","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since we have a vast amount of data, we will use solver=\"saga\" from logit and apply preprocessing of the input data using StandardScaler. class_weight=\"balanced\" and a regularization param C to the default value of 1","metadata":{"_uuid":"fbe5b699ec90474d3915f53fc8add240de227688","_cell_guid":"cbbfe093-adcb-4bb9-ba49-8ca2338a3803"}},{"cell_type":"code","source":"X_train_scaled = scaler.transform(X_train) #scaling features!\nX_val_scaled = scaler.transform(X_val)","metadata":{"_uuid":"f2f2e283bcc3870be919a861377618b2d33a6e26","collapsed":true,"_cell_guid":"82c86f08-13d4-42c2-ad4a-4a047e0b958b","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logit.fit(X_train_scaled, y_train)\nlogit_scores_proba = logit.predict_proba(X_train_scaled)\nlogit_scores = logit_scores_proba[:,1]","metadata":{"_uuid":"bdef4f0debf38d0c17fbb1fcaf0443dcbdc73357","collapsed":true,"_cell_guid":"df2012f6-fad2-4de9-9360-d53c5e48f342","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lets make a roc_curve visualization\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(12,10))\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1],[0,1], \"k--\")\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive rate\")","metadata":{"_uuid":"e4fa14dea5f355f97e61f4a1659a835436f27003","collapsed":true,"_cell_guid":"c3acfefc-a13a-4a6a-ab80-7f10682e600f","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fpr_logit, tpr_logit, thresh_logit = roc_curve(y_train, logit_scores)\nplot_roc_curve(fpr_logit,tpr_logit)\nprint(\"AUC Score {}\".format(roc_auc_score(y_train,logit_scores)))","metadata":{"_uuid":"8242b0000e1b3d062095ad9330b43d249f0bb32b","collapsed":true,"_cell_guid":"bc386418-90aa-4352-82b2-6a00f2463884","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#validate with the validation set\nlogit_scores_proba_val = logit.predict_proba(X_val_scaled)\nlogit_scores_val = logit_scores_proba_val[:,1]\nfpr_logit_val, tpr_logit_val, thresh_logit_val = roc_curve(y_val, logit_scores_val)\nplot_roc_curve(fpr_logit_val,tpr_logit_val)\nprint(\"AUC Score {}\".format(roc_auc_score(y_val,logit_scores_val)))","metadata":{"_uuid":"23718664d340366de1a9dc4c34f96354adf3a34a","collapsed":true,"_cell_guid":"6a95db96-ee93-4bd1-b83f-84a07c5553fe","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With using our first try with the logistic regression we got an AUC score of .80, not bad! let's try tuning the parameters to see if we can improve our score. we will try setting a different regularization factor, let's tighten it by 0.1 and 10. and making max_iteration to 1000. Our validation set score is not that far away from our training score and that's a good thing!","metadata":{"_uuid":"18830ace9131cbfba0c10ca362837e984cad15ee","_cell_guid":"cda18248-606a-4a9c-b017-b2299de3ac1f"}},{"cell_type":"code","source":"logit_C_low = LogisticRegression(random_state=42, solver=\"saga\", penalty=\"l1\", class_weight=\"balanced\", C=0.001, max_iter=1000)\nlogit_C_low.fit(X_train_scaled, y_train)\nlogit_C_low_scores_proba = logit_C_low.predict_proba(X_train_scaled)\nlogit_C_low_scores = logit_C_low_scores_proba[:,1]\nfpr_logit_C_low, tpr_logit_C_low, thresh_logit_C_low = roc_curve(y_train, logit_C_low_scores)\n#plot_roc_curve(fpr_logit_C_low,tpr_logit_C_low)\nprint(\"AUC Score {}\".format(roc_auc_score(y_train,logit_C_low_scores)))","metadata":{"_uuid":"f9a8eb1b51d48c7ec09490bcad2c04f965a88ef7","collapsed":true,"_cell_guid":"6a38ba46-f0cb-45ff-9067-d659450b9f04","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logit_C_high = LogisticRegression(random_state=42, solver=\"saga\", penalty=\"l1\", class_weight=\"balanced\", C=1000, max_iter=1000)\nlogit_C_high.fit(X_train_scaled, y_train)\nlogit_C_high_scores_proba = logit_C_high.predict_proba(X_train_scaled)\nlogit_C_high_scores = logit_C_high_scores_proba[:,1]\nfpr_logit_C_high, tpr_logit_C_high, thresh_logit_C_high = roc_curve(y_train, logit_C_high_scores)\nprint(\"AUC Score {}\".format(roc_auc_score(y_train,logit_C_high_scores)))","metadata":{"_uuid":"f9e0b98450e0f95dcd423e9173438a1da4d622d1","collapsed":true,"_cell_guid":"96ccc1f1-bc88-4292-8275-5ef88d149c67","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Lets visualize all of them at once!","metadata":{"_uuid":"087e75aeca239ef1f75cc6579088dcef8240a91f","_cell_guid":"d7e2ac87-a9d9-4cfc-be79-868bfdd82f98"}},{"cell_type":"code","source":"#lets make a roc_curve visualization\nplt.figure(figsize=(12,10))\nplt.plot(fpr_logit, tpr_logit, label=\"Logit C=1\")\nplt.plot(fpr_logit_C_high, tpr_logit_C_high , label=\"Logit C=1000\")\nplt.plot(fpr_logit_C_low, tpr_logit_C_low , label=\"Logit C=0.001\")\nplt.plot([0,1],[0,1], \"k--\", label=\"naive prediction\")\nplt.axis([0,1,0,1])\nplt.legend(loc=\"best\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive rate\")","metadata":{"_uuid":"a2e5d51f9d5055766570948d33c1eebe6d8e760c","collapsed":true,"_cell_guid":"53b2f2e3-79a5-43d0-ba80-d10b1dd28017","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Adjusting the C parameter don't mean much for our classifier to improve it's score. Let's try our second option which is to implement undersampling of our dataset to make the target variable balanced.","metadata":{"_uuid":"c0611e898121b32bfddb1213cdc3446933269a35","_cell_guid":"734f2971-872d-4e95-a6d6-973317ea27d4"}},{"cell_type":"code","source":"#Random Sampling\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\nprint(\"Original dataset shape {}\".format(Counter(y)))","metadata":{"_uuid":"b0b4e05701a0189083a0d1fcdc380f216257fad0","collapsed":true,"_cell_guid":"415dc251-6397-4c5c-acd8-25d77ac0da1f","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_sample(X,y)\nprint(\"Resampled dataset shape {}\".format(Counter(y_resampled)))","metadata":{"_uuid":"09b8c0b5dc6c92bb3601cd27fb385b6defe09f56","collapsed":true,"_cell_guid":"fcdb6d16-7f8e-4c7e-86b0-5b852aa298de","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From here, we dropped most of the majority class ended up on a 50/50 ratio. the disadvantage of this strategy is that have lost most of the information from the majority class. advantage are our dataset will have a faster training and we solved the unbalanced dataset problem. let's give it a try!","metadata":{"_uuid":"9dd6556f0b3578365fdb53cc61904e39530b2f24","_cell_guid":"fc1e42de-18da-4b73-8bdb-6f7b7878c709"}},{"cell_type":"code","source":"X_resampled.shape, y_resampled.shape","metadata":{"_uuid":"389cf8bfcce834044794bb73f5964e965b4baea6","collapsed":true,"_cell_guid":"a476ac45-bc59-4cd6-b052-2e2f32e2c40a","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_rus, X_val_rus, y_train_rus, y_val_rus = train_test_split(X_resampled, y_resampled, random_state=42)\nX_train_rus.shape, y_train_rus.shape","metadata":{"_uuid":"0a2208b175a0d16174498a5a44c5d4cc3467263d","collapsed":true,"_cell_guid":"c253057a-1341-489b-9835-5a1f5b1c300a","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler().fit(X_train_rus)\nX_train_rus_scaled = scaler.transform(X_train_rus)\nX_val_rus_scaled = scaler.transform(X_val_rus)","metadata":{"_uuid":"a77b320345f84ca01e6d82fc6bf31262fbfab6bd","collapsed":true,"_cell_guid":"bd6abed7-2d63-4f52-a4f3-41b9035b2f5f","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logit_resampled = LogisticRegression(random_state=42, solver=\"saga\", penalty=\"l1\", C=1.0, max_iter=500)\nlogit_resampled.fit(X_train_rus_scaled, y_train_rus)\nlogit_resampled_proba_res = logit_resampled.predict_proba(X_train_rus_scaled)\nlogit_resampled_scores = logit_resampled_proba_res[:,1]\nfpr_logit_resampled, tpr_logit_resampled, thresh_logit_resampled = roc_curve(y_train_rus, logit_resampled_scores)\nplot_roc_curve(fpr_logit_resampled,tpr_logit_resampled)\nprint(\"AUC Score {}\".format(roc_auc_score(y_train_rus, logit_resampled_scores)))","metadata":{"_uuid":"7c90a6bf640f6d4ac03d5459415e95527a424fc7","collapsed":true,"_cell_guid":"65f3c70c-2b2d-480b-8db5-4a513bb37c96","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our score doesn't improve that much using the undersampling method. One reason of this would be that the logisticregression model can't handle this vast amount of data or we have reached its limitation of predictive power on this type of dataset. Let's try other complex models!","metadata":{"_uuid":"e4aa95bbcb89a7304a0da951effa02f30d423417","_cell_guid":"3d142e96-2c9a-432c-b447-e86516cfe210"}},{"cell_type":"markdown","source":"One way to improve our score  is to use ensembling models.  First, we will use RandomForests and will try GradientBoostingClassifier and compare their scores.","metadata":{"_uuid":"9b9429a40ae79cc26225bbcbdd6aea5809595312","_cell_guid":"b9132f14-59d4-4012-8c09-d0fc5e287db4"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nforest = RandomForestClassifier(random_state=42,n_estimators=300, max_depth=5, class_weight=\"balanced\")\nforest.fit(X_train,y_train) #Using the original dataset, not the resampled\ny_scores_proba = forest.predict_proba(X_train)\ny_scores = y_scores_proba[:,1]\nfpr, tpr, thresh = roc_curve(y_train, y_scores)\nplot_roc_curve(fpr,tpr)\nprint(\"AUC Score {}\".format(roc_auc_score(y_train,y_scores))) #max_depth=5 .8525 #7 .864 cross .85 #10 .89 cross .85 #9 .88 cross .853 #12 .92 cross .84 Overfit!","metadata":{"_uuid":"689a8e18d7d3136e6cfe330bf717e054f044d3ed","collapsed":true,"_cell_guid":"a39c4551-fc49-4ea3-a93d-a27fe4c35ce1","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Let's cross validate\ny_val_proba = forest.predict_proba(X_val)\ny_scores_val = y_val_proba[:,1]\nfpr_val, tpr_val, thresh_val = roc_curve(y_val, y_scores_val)\nplot_roc_curve(fpr_val,tpr_val)\nprint(\"AUC Score {}\".format(roc_auc_score(y_val,y_scores_val)))","metadata":{"_uuid":"22b4e5d11d2036b88c3b9cf91f4bcd8511939376","collapsed":true,"_cell_guid":"6f22c660-ba5d-4a5c-b99f-8ae95676c693","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's see how the random forest classifier treat each of the features, here, the randomforest gives a huge importance for the CombinedDefaulted feature and the RevolvingUtilizationOfUnsecuredLines and almost disgregard the other features in its predictions.","metadata":{"_uuid":"378b6b3785fb6ba5094436975888d1f4f844dd8e","_cell_guid":"2a6295cc-384c-4776-993e-632a97876867"}},{"cell_type":"code","source":"def plot_feature_importances(model):\n    plt.figure(figsize=(10,8))\n    n_features = X.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X.columns)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)\n\nplot_feature_importances(forest)","metadata":{"_uuid":"5005f608126fec4058dbb90507fc2097de2bc7d0","collapsed":true,"_cell_guid":"f1e63fe6-43dc-442f-b5c8-dfcc39d3ea9c","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tuning the max_depth param of randomforest with class_weight gave us a good score of .87 in the training set and .85 in the validation set. \nHere are the summary of the param tuninng \n#7 .864 cross .85 #10 .89 cross .85 #9 .88 cross .853 #12 .92 cross .84 Overfit!","metadata":{"_uuid":"a617def124cf11ae7a2fd4bd68d8a7b48306f0d6","_cell_guid":"a144223c-3742-437d-8145-6b32000c500f"}},{"cell_type":"markdown","source":"Now Let's try GradientBoostingClassifier!","metadata":{"_uuid":"0c574c092ba12dbcc1e09f47cf5c2e7e1ca117a4","_cell_guid":"0d780404-7b69-4e26-86a1-f200b1c2ead3"}},{"cell_type":"code","source":"gbc_clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=8, random_state=42)\ngbc_clf.fit(X_train,y_train)\ngbc_clf_proba = gbc_clf.predict_proba(X_train)\ngbc_clf_scores = gbc_clf_proba[:,1]\nfpr_gbc, tpr_gbc, thresh_gbc = roc_curve(y_train, gbc_clf_scores)\nplot_roc_curve(fpr_gbc, tpr_gbc)\nprint(\"AUC Score {}\".format(roc_auc_score(y_train, gbc_clf_scores)))","metadata":{"_uuid":"48a6fb5a1fff2e1acde9e6d43599e4409e492e98","collapsed":true,"_cell_guid":"40cb84f3-8851-478e-83c1-e3d082c0f466","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#validation\ngbc_val_proba = gbc_clf.predict_proba(X_val)\ngbc_val_scores = gbc_val_proba[:,1]\nprint(\"AUC Score {}\".format(roc_auc_score(y_val, gbc_val_scores)))","metadata":{"_uuid":"b5b9cc8fdbc2142437f79712b8c05b57e4a812c3","collapsed":true,"_cell_guid":"fc555a04-67f9-45c9-a2c3-3f14b5f602a1","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We are overfitting! Let's try tuning the hyperparameters of our gradient boosting classifier to improve generalization.","metadata":{"_uuid":"46b108f0ad5d7760ffd9225abc96cad06298ba4e","_cell_guid":"4dc7eade-742a-45c0-b7d8-8a436e47105b"}},{"cell_type":"code","source":"gbc_clf_submission = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05 ,max_depth=4,  random_state=42)\ngbc_clf_submission.fit(X_train,y_train)\ngbc_clf_proba = gbc_clf_submission.predict_proba(X_train)\ngbc_clf_scores = gbc_clf_proba[:,1]\ngbc_val_proba = gbc_clf_submission.predict_proba(X_val)\ngbc_val_scores = gbc_val_proba[:,1]\nfpr_gbc, tpr_gbc, thresh_gbc = roc_curve(y_train, gbc_clf_scores)\nprint(\"AUC Score {}\".format(roc_auc_score(y_train, gbc_clf_scores))), print(\"AUC Score {}\".format(roc_auc_score(y_val, gbc_val_scores)))","metadata":{"_uuid":"2d85c3d53d25fb61f5ad8a3091e9509adf020556","collapsed":true,"_cell_guid":"2b568b11-886c-4c31-b5f4-ba24db7cdc99","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_feature_importances(gbc_clf)","metadata":{"_uuid":"962ed18cf279038e83c085b406b77d001e80f9f4","collapsed":true,"_cell_guid":"50808e47-f978-4b3a-a371-2a3e8cbc87aa","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, the GradientBoostingClassifier gives more emphasis on the RevolvingUtilizationOfUnsecuredLines feature and the DebtRatio, much more equally than the RandonForestClassifier. We're taking GradientBoostingClassifier as our model to submit on the kaggle competition.","metadata":{"_uuid":"fdc8aabb1d83d520357780c6bbd9185001eaad8f","_cell_guid":"60380fd7-9849-46f1-8168-1f576cd9ae0a"}},{"cell_type":"code","source":"X_test.shape","metadata":{"_uuid":"32355fdad8514f9d76cd50dc0424af0326bd9f3d","collapsed":true,"_cell_guid":"4168a8f2-30c6-4120-b609-b9494cdf8c44","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_proba = gbc_clf_submission.predict_proba(X_test)\nsubmission_scores = submission_proba[:,1] #Positive Class","metadata":{"_uuid":"e64a9d669ad762d1d7b8aeacf01e970d243711d7","collapsed":true,"_cell_guid":"f633ac01-826d-4589-9491-20b2651862fa","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids = np.arange(1,101504)\nsubmission = pd.DataFrame( {'Id': ids, 'Probability': submission_scores})\nsubmission.to_csv('submission_credit.csv', index=False)","metadata":{"_uuid":"be404df45b90753ae3ce2da8fb6f2b3d2e6017c8","collapsed":true,"_cell_guid":"69ef6822-4863-41c0-b089-9944f0b8239b","trusted":false,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}